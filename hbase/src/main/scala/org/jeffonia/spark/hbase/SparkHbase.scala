package org.jeffonia.spark.hbase

/**
 * Created by jeffonia on 2016/6/25.
 */
object SparkHbase extends Serializable {
  final val cfStatsBytes = Bytes.toBytes("f")

  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: NetworkWordCount <hostname> <port>")
      System.exit(1)
    }

    // Create the context with a 1 second batch size
    val sparkConf = new SparkConf().setAppName("NetworkWordCount").setMaster("local[2]")
    val ssc = new StreamingContext(sparkConf, Seconds(10))

    // Hbase configuration, bla, bla, bla
    val List(hbase_table, columnFamily, column) = List("jeff", "f", "count")
    val hbaseWriterConf = HBaseFactory.getWriterConf(hbase_table)
    val hbaseScanner = HBaseFactory.getScannerConf(hbase_table, columnFamily, column)

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).filter(x => x._1.length > 0).reduceByKey(_ + _).cache

    /**
     * Write to hbase
     */
    wordCounts.foreachRDD(rdd => {
      println("Write to hbase count: " + rdd.count)
      rdd.map { case (k, v) => convertToPut(columnFamily, (k, v)) }.saveAsHadoopDataset
      (hbaseWriterConf)
    })
    wordCounts.print

    /**
     * Read to hbase
     */
    val sparkContext = ssc.sparkContext
    val hbaseRdd = sparkContext.newAPIHadoopRDD(hbaseScanner, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result]).cache
    println("Loaded HBase rdd count: " + hbaseRdd.count)
    hbaseRdd.foreach { case (_, result) =>
      val rowKey = Bytes.toString(result.getRow)
      val columnValue = Bytes.toString(result.getValue(columnFamily.getBytes, column.getBytes))
      println("Row key:" + rowKey + ", " + column + ": " + columnValue)
    }

    ssc.start()
    ssc.awaitTermination()
  }

  def convertToPut(columnFamily: String, tuple: (String, Int)): (ImmutableBytesWritable, Put) = {
    val p = new Put(Bytes.toBytes(tuple._1))
    // add columns with data values to put
    p.add(Bytes.toBytes(columnFamily), Bytes.toBytes("count"), Bytes.toBytes(tuple._2.toString))
    println("Write to hbase with rowkey: " + tuple._1 + ", column name: count, value: " + tuple._2)
    (new ImmutableBytesWritable, p)
  }

}
